ä»»åŠ¡ï¼šé€šè¿‡ä¿®æ”¹åŸå§‹æ¨¡å‹çš„å‚æ•°é…ç½®ï¼ˆå¦‚å±‚æ•°ã€éšè—å±‚å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼‰ï¼Œä½¿å…¶æ¨¡å‹ä½“ç§¯æ˜¾è‘—ç¼©å°ï¼Œè¾¾åˆ° 10MB+ çš„ç›®æ ‡ã€‚
## âœ… æ€»ä½“æ€è·¯

ä½ å°†ä½¿ç”¨ `transformers` æä¾›çš„ API æ¥ï¼š

1. **å®šä¹‰ä¸€ä¸ªæ›´å°çš„æ¨¡å‹é…ç½®ï¼ˆConfigï¼‰**
2. **åŸºäºè¯¥é…ç½®éšæœºåˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹**
3. **ä¿å­˜è¿™ä¸ªå°æ¨¡å‹**
4. ï¼ˆå¯é€‰ï¼‰åŠ è½½å¹¶éªŒè¯æ¨¡å‹ç»“æ„å’Œå¤§å°

è¿™æ ·ä½ å°±èƒ½å¾—åˆ°ä¸€ä¸ª**å‚æ•°é‡è¿œå°äºåŸå§‹å¤§æ¨¡å‹çš„å°æ¨¡å‹**ã€‚

---

## ğŸ§± ä¸€ã€é¡¹ç›®ç»“æ„å»ºè®®ï¼ˆæ›´æ–°ï¼‰

ç”±äºä½ ç°åœ¨ä¸»è¦æ˜¯é€šè¿‡ä¿®æ”¹é…ç½®æ¥ç”Ÿæˆå°æ¨¡å‹ï¼Œå¯ä»¥ç®€åŒ–ä¹‹å‰çš„ç›®å½•ç»“æ„ä¸ºå¦‚ä¸‹å½¢å¼ï¼š

```
tiny-transformers/
â”‚
â”œâ”€â”€ configs/                      # å­˜æ”¾ä¸åŒæ¨¡å‹é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ tiny_bert_config.json     # è‡ªå®šä¹‰çš„å°å‹BERTé…ç½®
â”‚   â””â”€â”€ tiny_gpt2_config.json
â”‚
â”œâ”€â”€ models/                       # ç”¨äºç”Ÿæˆå’Œä¿å­˜æ¨¡å‹çš„è„šæœ¬
â”‚   â”œâ”€â”€ generate_tiny_model.py    # ä¸»è„šæœ¬ï¼šè¯»å–é…ç½®ã€ç”Ÿæˆå¹¶ä¿å­˜æ¨¡å‹
â”‚
â”œâ”€â”€ utils/                        # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ model_size_utils.py       # æ¨¡å‹å¤§å°ç»Ÿè®¡å·¥å…·
â”‚
â”œâ”€â”€ saved_models/                 # ä¿å­˜çš„å°æ¨¡å‹
â”‚   â”œâ”€â”€ tiny-bert/
â”‚   â””â”€â”€ tiny-gpt2/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“¦ äºŒã€ç¤ºä¾‹ä»£ç å®ç°

### 1. å®šä¹‰ä¸€ä¸ªå°å‹ BERT é…ç½®ï¼ˆ`configs/tiny_bert_config.json`ï¼‰

ä½ å¯ä»¥æ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªæ¯”åŸå§‹ `bert-base-uncased` å°å¾ˆå¤šçš„é…ç½®æ–‡ä»¶ï¼š

```json
{
  "model_type": "bert",
  "vocab_size": 30522,
  "hidden_size": 128,           // åŸå§‹æ˜¯ 768
  "num_hidden_layers": 4,       // åŸå§‹æ˜¯ 12
  "num_attention_heads": 4,     // åŸå§‹æ˜¯ 12
  "intermediate_size": 512,     // åŸå§‹æ˜¯ 3072
  "max_position_embeddings": 512,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "attention_probs_dropout_prob": 0.1,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "use_cache": true
}
```

---

### 2. ç”Ÿæˆå¹¶ä¿å­˜å°æ¨¡å‹çš„ä¸»è„šæœ¬ï¼ˆ`models/generate_tiny_model.py`ï¼‰

```python
from transformers import BertConfig, BertForMaskedLM
import os
import json

def load_config(config_path):
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    return BertConfig(**config_dict)

def main():
    config_path = "../configs/tiny_bert_config.json"
    output_dir = "../saved_models/tiny-bert"

    config = load_config(config_path)
    print("Model Config:\n", config)

    # åˆå§‹åŒ–ä¸€ä¸ªåŸºäºè¯¥é…ç½®çš„å°æ¨¡å‹ï¼ˆéšæœºæƒé‡ï¼‰
    model = BertForMaskedLM(config)

    # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total Parameters: {total_params / 1e6:.2f}M")

    # ä¿å­˜æ¨¡å‹
    os.makedirs(output_dir, exist_ok=True)
    model.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")

if __name__ == "__main__":
    main()
```

---

### 3. æŸ¥çœ‹æ¨¡å‹å¤§å°çš„å·¥å…·ï¼ˆ`utils/model_size_utils.py`ï¼‰

```python
import os
import torch

def get_model_size(model):
    """
    è·å–æ¨¡å‹å ç”¨ç£ç›˜ç©ºé—´å¤§å°ï¼ˆMBï¼‰
    """
    with tempfile.TemporaryDirectory() as tmp_dir:
        model_path = os.path.join(tmp_dir, "model.pt")
        torch.save(model.state_dict(), model_path)
        size_mb = os.path.getsize(model_path) / (1024 * 1024)
    return round(size_mb, 2)
```

åœ¨ä¸»è„šæœ¬ä¸­åŠ å…¥è°ƒç”¨å³å¯æ˜¾ç¤ºä¿å­˜åçš„æ¨¡å‹æ–‡ä»¶å¤§å°ã€‚

---

## ğŸ“ˆ ä¸‰ã€é¢„æœŸæ•ˆæœå¯¹æ¯”

| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | å‚æ•°é‡ | æ–‡ä»¶å¤§å° |
|------|------|-----------|--------|----------|
| `bert-base` | 12 | 768 | ~110M | ~400MB |
| **tiny-bert** | 4 | 128 | ~1.5M | **~6MB** |

âœ… è¿™æ ·ä½ å°±å¯ä»¥è½»æ¾å¾—åˆ°ä¸€ä¸ª **å°äº 10MB çš„å°æ¨¡å‹**ï¼

---

## ğŸ§© å››ã€åç»­æ‰©å±•æ–¹å‘ï¼ˆå¯é€‰ï¼‰

è™½ç„¶ä½ ç°åœ¨åªæ˜¯ä¿®æ”¹é…ç½®ï¼Œä½†æœªæ¥ä½ å¯ä»¥å°è¯•ï¼š

- ä½¿ç”¨ `AutoModel.from_config()` æ”¯æŒå¤šç§æ¨¡å‹ï¼ˆGPT2ã€RoBERTa ç­‰ï¼‰
- æ·»åŠ è®­ç»ƒè„šæœ¬è¿›è¡Œå¾®è°ƒ
- ä½¿ç”¨ `safetensors` æ ¼å¼ä¿å­˜ï¼Œè¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ä½“ç§¯
- ç¼–å†™æ¨¡å‹æ¨ç†è„šæœ¬
- æ„å»º Web Demoï¼ˆå¦‚ Gradioï¼‰

---

## ğŸ“ äº”ã€README ç¤ºä¾‹å†…å®¹

```markdown
# Tiny Transformers

è¯¥é¡¹ç›®æ—¨åœ¨é€šè¿‡è°ƒæ•´ HuggingFace Transformers ä¸­çš„æ¨¡å‹é…ç½®ï¼Œç”Ÿæˆä½“ç§¯å°äº 10MB çš„å°æ¨¡å‹ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒæˆ–æ•™å­¦æ¼”ç¤ºã€‚

## å½“å‰æˆæœ

- âœ… æˆåŠŸç”Ÿæˆä¸€ä¸ª 6MB å·¦å³çš„ Tiny-BERT æ¨¡å‹
- âœ… å¯æ‰©å±•æ”¯æŒ GPT2ã€DistilBERT ç­‰æ¶æ„

## å¦‚ä½•è¿è¡Œ

```bash
cd models
python generate_tiny_model.py
```

ç”Ÿæˆçš„æ¨¡å‹ä½äº `saved_models/` ç›®å½•ä¸‹ã€‚
```

---

## âœ… å…­ã€æ€»ç»“

ä½ ç°åœ¨è¦åšçš„å°±æ˜¯ï¼š

1. åˆ›å»ºä¸Šè¿°é¡¹ç›®ç»“æ„
2. ç¼–å†™é…ç½®æ–‡ä»¶ï¼ˆJSONï¼‰
3. ç¼–å†™ç”Ÿæˆæ¨¡å‹çš„ Python è„šæœ¬
4. ä¿å­˜å¹¶æŸ¥çœ‹æ¨¡å‹å¤§å°
5. å¤šä¸ªæ¨¡å‹å¯ä»¥åˆ†åˆ«å†™æˆå¤šä¸ª `.py` æ–‡ä»¶ï¼ˆå¦‚ `generate_tiny_gpt2.py`ï¼‰

---

å¦‚æœä½ å¸Œæœ›æˆ‘ä¸ºä½ ç”Ÿæˆå®Œæ•´çš„é¡¹ç›®æ¨¡æ¿ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ–‡ä»¶ç»“æ„ + ç¤ºä¾‹ä»£ç ï¼‰ï¼Œè¯·å‘Šè¯‰æˆ‘ ğŸ‘ æˆ‘å¯ä»¥ä¸€æ¬¡æ€§æ‰“åŒ…å‘ç»™ä½ ï¼Œæ–¹ä¾¿ä½ ç›´æ¥å¼€å§‹å¼€å‘ã€‚

ç¥ä½ æ—©æ—¥å®Œæˆè‡ªå·±çš„â€œå°æ¨¡å‹å·¥ç¨‹â€ï¼éœ€è¦ä»»ä½•å…·ä½“å¸®åŠ©éƒ½å¯ä»¥ç»§ç»­é—®æˆ‘ ğŸ˜Š